{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht-ThfGZ6rcz"
      },
      "source": [
        "# Pathology Deep Learning Hands-On\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KatherLab/stamp_demo/blob/master/stamp_hands_on.ipynb)\n",
        "\n",
        "Welcome to the 2025 Clinicum Digitale digital pathology hands-on session.\n",
        "In this session we will have a look at what a typical machine learning workflow in our lab looks like.\n",
        "We will predict the TP53 gene alteration in breast cancer from histopathologic whole-slide images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV8acwCned1u"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RVcE8dqn5haW"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from tqdm.notebook import tqdm\n",
        "from pathlib import Path\n",
        "import hashlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl9Y_hE2htyh"
      },
      "source": [
        "## 1. Change runtime type\n",
        "First, switch to a GPU-enabled Colab runtime: within Google Colab, go to *Runtime* $\\to$ *Change runtime type*, and select **, as indicated in the screenshot below.\n",
        "\n",
        "<img src=\"https://github.com/georg-wolflein/stamp_hands_on/blob/main/docs/colab_runtime.png?raw=true\" width=500 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmRLQ52UiVMN"
      },
      "source": [
        "## 2. Download data\n",
        "Let's download our dataset of extracted features. This will take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8ceab2f2dfff46c7a5287f0f58a3a9cc",
            "761ce4e94ba94d688f50a6e2f6ad293d",
            "54863c12b1ed4909aa4aa79a47ddcf4f",
            "71393d4c10cb4276ba05dc18fb64695b",
            "23f11454fc4e4b369d16cb20631db175",
            "5026eb1b0a694af8a851b1983aa2e082",
            "2522bcd0dcba49649f54dbdd5488881e",
            "fa1f4d1ec7364273a25d9ddca2b578c3",
            "eea8f60c23dd4402aaa48e8e6774985a",
            "f57c33d093924e138cb5ca72aba53d15",
            "da80f9053d0145d991fe742afd0e5b96"
          ]
        },
        "id": "ZyPzxx-8iYHV",
        "outputId": "05c2b091-7c27-45bf-8af1-aea8bd38a825"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TCGA_BRCA_10x_UNI_features.tar.gz.part_aa:   0%|          | 0.00/1.30G [00:00<?, ?iB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ceab2f2dfff46c7a5287f0f58a3a9cc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "_DOWNLOAD_PARTS = {\n",
        "    \"TCGA_BRCA_10x_UNI_features.tar.gz.part_aa\": \"6ff1600f3dcdc6344d3a5c46eca481c4\",\n",
        "    \"TCGA_BRCA_10x_UNI_features.tar.gz.part_ab\": \"7b4c7bb21ac365ee86be86e10f6e4efa\",\n",
        "}\n",
        "\n",
        "\n",
        "def md5(fname: str, chunk_size=8192) -> str:\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(fname, \"rb\") as f:\n",
        "        while chunk := f.read(chunk_size):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "\n",
        "def download(url: str, output_file: Path, checksum: str, chunk_size=1024):\n",
        "    if output_file.exists():\n",
        "        if md5(output_file) == checksum:\n",
        "            print(f\"{output_file} already downloaded, skipping...\")\n",
        "            return\n",
        "        else:\n",
        "            output_file.unlink()\n",
        "\n",
        "    resp = requests.get(url, stream=True)\n",
        "    total = int(resp.headers.get(\"content-length\", 0))\n",
        "    with (\n",
        "        output_file.open(\"wb\") as f,\n",
        "        tqdm(\n",
        "            desc=str(output_file),\n",
        "            total=total,\n",
        "            unit=\"iB\",\n",
        "            unit_scale=True,\n",
        "            unit_divisor=1024,\n",
        "        ) as bar,\n",
        "    ):\n",
        "        for data in resp.iter_content(chunk_size=chunk_size):\n",
        "            size = f.write(data)\n",
        "            bar.update(size)\n",
        "\n",
        "\n",
        "if not Path(\"TCGA_BRCA_10x_UNI_features.tar.gz\").exists():\n",
        "    for filename, checksum in _DOWNLOAD_PARTS.items():\n",
        "        download(\n",
        "            f\"https://github.com/KatherLab/stamp_demo/releases/download/data-release/{filename}\",\n",
        "            Path(filename),\n",
        "            checksum,\n",
        "        )\n",
        "\n",
        "    !cat TCGA_BRCA_10x_UNI_features.tar.gz.part_* > TCGA_BRCA_10x_UNI_features.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uX0KBKV9cGM"
      },
      "source": [
        "Now, let's extract the tar archive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74bESBtl8gF-"
      },
      "outputs": [],
      "source": [
        "!test -d TCGA_BRCA_10x_UNI_features || \\\n",
        "    (mkdir -p TCGA_BRCA_10x_UNI_features && \\\n",
        "     tar -xzf TCGA_BRCA_10x_UNI_features.tar.gz -C TCGA_BRCA_10x_UNI_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI6lCMeo-DeS"
      },
      "source": [
        "As a sanity check, ensure there are exactly `242` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4VKjUn7-IAF"
      },
      "outputs": [],
      "source": [
        "!ls TCGA_BRCA_10x_UNI_features | wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxkfEHHTDgOx"
      },
      "source": [
        "Finally, we will download the metadata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2tgNtonho7l"
      },
      "source": [
        "## 3. Install dependencies\n",
        "Here, we will install [STAMP](https://github.com/KatherLab/STAMP), a pipeline for computational pathology developed in [our lab](https://jnkather.github.io/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DloOBewi1qkI"
      },
      "outputs": [],
      "source": [
        "!pip install \"stamp @ git+https://github.com/KatherLab/STAMP\"\n",
        "#!pip install \"stamp @ git+https://github.com/georg-wolflein/STAMP@dev/colab\"\n",
        "!pip uninstall -y pandas numpy && pip install pandas numpy --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgRN1KCiyWmp"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G37hHEMyX8o"
      },
      "source": [
        "## From whole slide image to classification output\n",
        "\n",
        "In this notebook, we will train a deep learning model to classify whole slide images (WSIs).\n",
        "To do this, we follow the following structure:\n",
        "$$\n",
        "\\mathbb{R}^{H \\times W \\times 3}\n",
        "\\xrightarrow{\\text{tiling}} \\mathbb{R}^{n \\times p \\times p \\times 3}\n",
        "\\xrightarrow{\\text{extract features}} \\mathbb{R}^{n \\times d}\n",
        "\\xrightarrow{\\text{aggregate}} \\mathbb{R}^{1 \\times d}\n",
        "\\xrightarrow{\\text{classify}} [0, 1]\n",
        "$$\n",
        "where $H,W$ are the dimensions of the original whole slide image,\n",
        "$p=224$ is the patch size, and\n",
        "$d=1024$ is the dimensionality of the feature extractor.\n",
        "\n",
        "<img src=\"https://github.com/georg-wolflein/good-features/blob/master/assets/overview.png?raw=true\" width=800 />\n",
        "\n",
        "\n",
        "For simplicity, we will only do the **downstream training** part in this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyiq7jdxIiVu"
      },
      "source": [
        "## The structure of our data\n",
        "\n",
        "Let's first have a look at our data.\n",
        "The dataset we are using today consists of three major components:\n",
        "\n",
        "1. The clini table contains clinical data for each patient\n",
        "2. The slide table maps each slide a patient\n",
        "3. The slide features contain a condensed, machine-learning-ready representation of the slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAERMAKiEQFg"
      },
      "source": [
        "### Clini table\n",
        "\n",
        "The clini table contains clinical information for each patient.\n",
        "Each row of the clini table describes exactly one patient.\n",
        "\n",
        "* The column `PATIENT` contains a patient ID in the form `TCGA-site-patient` (`site` tells us which hostpital the patient is from)\n",
        "* The remaining columns contain other clinical information on the patient\n",
        "  * Among these, the `TP53` column indicates if there is a mutation of TP53. We will try to predict this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCh9kid32Cvp"
      },
      "outputs": [],
      "source": [
        "!test -f TCGA-BRCA-DX_CLINI.csv || wget https://raw.githubusercontent.com/KatherLab/stamp_demo/refs/heads/master/TCGA-BRCA-DX_CLINI.csv -q -O TCGA-BRCA-DX_CLINI.csv\n",
        "\n",
        "import pandas as pd\n",
        "clini_df = pd.read_csv(\"TCGA-BRCA-DX_CLINI.csv\")\n",
        "clini_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN7yHS3-J8Cd"
      },
      "source": [
        "### Slide table\n",
        "We often have multiple slides per patient.\n",
        "The slide table matches each slide to its patient.\n",
        "If a patient has multiple slides, it will appear multiple times, once for each slide they have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-87kpi_Vx1t"
      },
      "outputs": [],
      "source": [
        "!test -f TCGA-BRCA-DX_SLIDE.csv || wget https://raw.githubusercontent.com/KatherLab/stamp_demo/refs/heads/master/TCGA-BRCA-DX_SLIDE.csv -q -O TCGA-BRCA-DX_SLIDE.csv\n",
        "slide_df = pd.read_csv(\"TCGA-BRCA-DX_SLIDE.csv\")\n",
        "slide_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCvBJl8LKRz_"
      },
      "outputs": [],
      "source": [
        "for _, row in (\n",
        "    slide_df.groupby(\"PATIENT\").nunique().value_counts().reset_index().iterrows()\n",
        "):\n",
        "    print(f\"{row['count']:3d} patients have {row['FILENAME']} slides\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zucMb51BLH72"
      },
      "source": [
        "### Features\n",
        "Finally, we have the slide features themselves.\n",
        "Since whole slide images are large, too large to do machine learning on them directly, we first reduce them to a more managable form with a feature extractor.\n",
        "The feature extractor is itself a neural network.\n",
        "It takes a whole slide image and reduces it to a more condensed form.\n",
        "While the exact mechanism by which it does so is outside the scope of this course, it compresses the size of an input 10-fold, allowing us to use neural networks to analyze them.\n",
        "\n",
        "Since this process does take quite some time, we have already extracted the features for today's dataset in advance.\n",
        "Let's have a look at the features for one particular whole slide image.\n",
        "\n",
        "Below is a thumbnail of this WSI. We removed background areas (shown in red).\n",
        "\n",
        "![Slide Image](https://raw.githubusercontent.com/KatherLab/stamp_demo/refs/heads/master/TCGA-BH-A0HU-01Z-00-DX1.73B38904-E4F8-4F45-BD75-A27EC833B6DE.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV2jhI-m5haY"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "with h5py.File(\n",
        "    \"TCGA_BRCA_10x_UNI_features/TCGA-BH-A0HU-01Z-00-DX1.73B38904-E4F8-4F45-BD75-A27EC833B6DE.h5\",\n",
        "    \"r\",\n",
        ") as f:\n",
        "    feats = f[\"feats\"][:]\n",
        "    coords = f[\"coords\"][:]\n",
        "    print(\"Shape of features array:\", feats.shape)\n",
        "    print(\"Shape of coordinates array:\", coords.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mBHx-XL5haY"
      },
      "source": [
        "As we can see, we have $n=5597$ feature vectors in this slide. This means that the WSI was split into 5597 patches. From each patch, we extracted a $d=1024$ dimensional feature vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKsiPRNe5haY"
      },
      "source": [
        "#### Features\n",
        "Let's have a look at one of the feature vectors. As we will see, it consists of 1024 floating point numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwAsedNl5haY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(feats[0])\n",
        "plt.figure(figsize=(10, 2))\n",
        "plt.bar(range(1024), feats[0])\n",
        "plt.xlabel(\"Feature dimension\")\n",
        "plt.ylabel(\"Feature value\")\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8dW_7o35haY"
      },
      "source": [
        "#### Coordinates\n",
        "Let's also visualize the coordinates of the patches. We will see the shape of the WSI. Compare this to the image of the WSI above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFhBLhy95haY"
      },
      "outputs": [],
      "source": [
        "plt.plot(coords[:, 0], coords[:, 1], \"o\", markersize=1)\n",
        "plt.xlabel(\"x coordinate\")\n",
        "plt.ylabel(\"y coordinate\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.axis(\"equal\")\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqQ9UeAs5haY"
      },
      "source": [
        "## Splitting our data\n",
        "\n",
        "We will split our dataset into two subsets: one part for training, one for testing.\n",
        "Specifically, we will use patients from the largest site as the training set and the second largest site as the test set.\n",
        "\n",
        "Often times, pathological slides contain artifacts like staining differences that make it possible to infer where slides originate from.\n",
        "If certain hospitals have for example a higher rate of severe cases, the network may be base its prediction based on these artifacts instead of actually medically relevant features.\n",
        "By ensuring that our testing set is from another site, we will be able to determine if our network is able to generalize to new sites."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov7uV1vb5haY"
      },
      "outputs": [],
      "source": [
        "site = clini_df[\"PATIENT\"].str.split(\"-\", expand=True)[1]\n",
        "train_clini_df = clini_df[site == \"BH\"]\n",
        "test_clini_df = clini_df[site == \"A2\"]\n",
        "\n",
        "train_clini_df.to_csv(\"TCGA-BRCA-DX_CLINI_train.csv\", index=False)\n",
        "test_clini_df.to_csv(\"TCGA-BRCA-DX_CLINI_test.csv\", index=False)\n",
        "\n",
        "print(f\"Training set: {len(train_clini_df)} patients\")\n",
        "print(f\"Testing set: {len(test_clini_df)} patients\")\n",
        "train_clini_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q0B-j9R-Jnx"
      },
      "source": [
        "## Inspecting our data\n",
        "\n",
        "Before starting training any models, it is often worth it to inspect the data to ensure that there are no glaring problems with it.\n",
        "Let's look at the TP53 column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nYYi2cl-rNa"
      },
      "outputs": [],
      "source": [
        "train_clini_df[\"TP53\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in7cubn7_bln"
      },
      "source": [
        "As we can see, one class is less frequent than the other.\n",
        "This can lead to problems while training our network.\n",
        "Why becomes intuitively apparent if you consider a strongly imbalanced dataset with only two classes, one making up 90% of the dataset.\n",
        "The network can trivially reach an accuracy of 90% by just always chosing the more frequent class.\n",
        "\n",
        "One approach to combat this is to weigh the classes.\n",
        "In STAMP, the classes are automatically weighed in such a way that each class has the overall same contribution.\n",
        "For a nine-to-one imbalanced two-class dataset, each instance of the rare class would thus be weighted as having nine times the importance of a sample of the more common class.\n",
        "\n",
        "This can of course still lead to instabilities in training, especially if one of the rare classes is one we don't particularly care about.\n",
        "In that case the network may spend too much time learning how to correctly classify the unimportant class at the cost of more interesting classes.\n",
        "\n",
        "However, we should be fine here because the imbalance isn't too severe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSlT5QpkLg7G"
      },
      "source": [
        "## Training a Model\n",
        "\n",
        "We will now train our model.\n",
        "\n",
        "Let's store our results in a folder named the same as our target, i.e. \"Grade without G1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_66kmOodh02"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "output_dir = Path(\"output\")\n",
        "output_dir.mkdir(exist_ok=True, parents=True)\n",
        "output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8awOFfXuPgit"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from stamp.modeling.train import train_categorical_model_\n",
        "import torch\n",
        "\n",
        "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "train_result = train_categorical_model_(\n",
        "    output_dir=output_dir,\n",
        "    clini_table=Path(\"TCGA-BRCA-DX_CLINI_train.csv\"),\n",
        "    slide_table=Path(\"TCGA-BRCA-DX_SLIDE.csv\"),\n",
        "    feature_dir=Path(\"TCGA_BRCA_10x_UNI_features\"),\n",
        "    patient_label=\"PATIENT\",\n",
        "    ground_truth_label=\"TP53\",\n",
        "    filename_label=\"FILENAME\",\n",
        "    categories=[\"0\", \"1\"],\n",
        "    # Dataset and -loader parameters\n",
        "    bag_size=128, # 512\n",
        "    num_workers=min(os.cpu_count() or 1, 16),\n",
        "    # Training paramenters\n",
        "    batch_size=1, # 64\n",
        "    max_epochs=64,\n",
        "    patience=16,\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "    # Experimental features\n",
        "    use_vary_precision_transform=False,\n",
        "    use_alibi=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stamp\n",
        "stamp.__file__"
      ],
      "metadata": {
        "id": "3FOhfjtXbD3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tpu-info"
      ],
      "metadata": {
        "id": "S6X2pCSqXgDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "T7e76Fl1-yfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLMGmLoGICuX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lightning_logs_dir = sorted(output_dir.joinpath(\"lightning_logs\").glob(\"version_*\"))[-1]\n",
        "\n",
        "history = pd.read_csv(lightning_logs_dir / \"metrics.csv\")\n",
        "history = history.groupby([\"epoch\", \"step\"]).first().reset_index()\n",
        "history.plot(x=\"epoch\", y=[\"training_loss\", \"validation_loss\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1jWW6FRpN--"
      },
      "source": [
        "As we can see, the training loss decreases a lot faster and further than the validation loss.\n",
        "This is to be expected:\n",
        "since the network is trained on the training set, it does not only learn to recognize features relevant for classifying the target, but also learns to recognize the training images themselves.\n",
        "\n",
        "Many of the features the network learns will not generalize.\n",
        "In general, the longer we train a network, the more likely it is that it will pick up small, non-generalizing details uniquely identifying a singular image from the training set.\n",
        "This is why we have a validation set:\n",
        "By checking how well the network performs on the validation set, we can determine whether the network is still learning generalizable features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwGzdSlKIfyK"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_history.roc_auc_score)\n",
        "plt.title(\"Validation ROC AUC Score\")\n",
        "plt.xlabel(\"Epoch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZgzfYQ6sHt4"
      },
      "source": [
        "The same should be visible in the AUROC over the progress of the training:\n",
        "initially, the ROC AUC score on the validation drops sharply while the network learns to recognize well-generalizing featues.\n",
        "Then, as these easy-to-recognize features have been exhausted, improvement quickly becomes slower and stagnates.\n",
        "\n",
        "If we train for too long, the performance on the validation set may even regress, as the only thing the network is doing during training is learning how to best classify the training set and one way of doing that is to just \"memorize\" all the specific training samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rplge8F7TdWw"
      },
      "source": [
        "## Validating our Model's Performance on the Internal Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IYwhupq1FSe"
      },
      "outputs": [],
      "source": [
        "train_result.patient_preds_df.to_csv(output_dir / \"valid_preds.csv\", index=False)\n",
        "train_result.patient_preds_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCsZval44JtH"
      },
      "source": [
        "As you can see, the neural network actually doesn't give us a decision, but a probability for our classes.\n",
        "Depending on what we use the network for, it may actually be useful to select a higher or lower threshold:\n",
        "for a screening test for example we may use a very low threshold to ensure that we definitely include all patients that have a specific illness.\n",
        "\n",
        "One tool that can help us qualify the quality of our classifier is the Receiver-Operator-Characteristic Curve, or ROC-Curve for short:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s-ZqXJ9bBS4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wanshi.visualizations.roc import plot_single_decorated_roc_curve\n",
        "\n",
        "true_class = \"G2\"\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "plot_single_decorated_roc_curve(\n",
        "    ax,\n",
        "    y_true=train_result.patient_preds_df[target_label] == true_class,\n",
        "    y_score=train_result.patient_preds_df[f\"{target_label}_{true_class}\"],\n",
        "    n_bootstrap_samples=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7dPLLMW0sON"
      },
      "source": [
        "The ROC curve plots the true positive rate (also called specificity) against the false positive rate (1 - sensitivity).\n",
        "We can evidently force our classifier to have perfect sensitivity by classifying _every_ sample as positive (i.e. setting the classification to 0).\n",
        "Similarly, we can make its specificity perfect by classifying every sample as false.\n",
        "Clearly, these classifiers are not particularly useful.\n",
        "The ROC curve shows us, how sensitivity and specificity fluctuate for different cutoffs.\n",
        "The area under that curve (AUC) is often used as a quick-and-easy way to compare classifiers' performance.\n",
        "For a more detailed explanation, check out [this visual explanation of ROC curves](https://mlu-explain.github.io/roc-auc/).\n",
        "\n",
        "Similarly, we can plot the sensitivity and specificity for each threshold and use that graph to dermine which cutoff we ought to choose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PEX3I6P1aCp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "y_true = train_result.patient_preds_df[target_label] == true_class\n",
        "y_score = train_result.patient_preds_df[f\"{target_label}_{true_class}\"]\n",
        "\n",
        "valid_fpr, valid_tpr, valid_thresholds = roc_curve(\n",
        "    y_true=y_true,\n",
        "    y_score=y_score,\n",
        ")\n",
        "valid_sensitivity = valid_tpr\n",
        "valid_specificity = 1 - valid_fpr\n",
        "\n",
        "plt.plot(valid_thresholds[1:], valid_sensitivity[1:], label=\"sensitivity\")\n",
        "plt.plot(valid_thresholds[1:], valid_specificity[1:], label=\"specificity\")\n",
        "plt.xlabel(\"threshold\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EkAOkI4RUgS"
      },
      "source": [
        "# Deploing our model on external data\n",
        "\n",
        "**As soon as we use the testing set, we are not allowed to change the experimental setup any more**.\n",
        "It is, for example, not valid to drop a category instead of merging it with another is not admissable, as we would effectively fine-tune the problem statement to the testing set if we do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQG9Mk0oRRwP"
      },
      "outputs": [],
      "source": [
        "!test -f tcga-stad-test-clini.csv || wget http://owlbear.maruchan.de:8000/clinicum_digitale_2023/tcga-stad-test-clini.csv\n",
        "import pandas as pd\n",
        "\n",
        "test_clini_df = pd.read_csv(\"tcga-stad-test-clini.csv\")\n",
        "test_clini_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga3Ixmk5PNrn"
      },
      "source": [
        "We of course have to transform the testing data the same way we transformed the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMB3EQgcRwUi"
      },
      "outputs": [],
      "source": [
        "# copy all values to a new column\n",
        "test_clini_df[\"Grade without G1\"] = test_clini_df[\"Grade\"]\n",
        "# but drop the ones which are \"G1\"\n",
        "test_clini_df.loc[train_clini_df[\"Grade\"] == \"G1\", \"Grade without G1\"] = None\n",
        "\n",
        "test_clini_df[\"Grade without G1\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TVV-V0ePajg"
      },
      "source": [
        "We will now _deploy_ our model on the testing set, that is, see how well it can predict never-before seen data.\n",
        "This is different from our validation set in that, while the network was not _trained_ on the validation data, we did determine which epoch's model was the best based on the validation set.\n",
        "Furthermore, parts of the validation set were sampled from the same cohorts as the training set.\n",
        "We can thus expect the validation set to be more akin to the training set, and thus expect the network to perform better for the validation set than the testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cETrcL5mfulJ"
      },
      "outputs": [],
      "source": [
        "from marugoto.mil.deploy import deploy_from_clini_slide\n",
        "\n",
        "test_patient_preds_df = deploy_from_clini_slide(\n",
        "    learn=train_result.learn,\n",
        "    clini_table=test_clini_df,\n",
        "    slide_table=slide_df,\n",
        "    feature_dir=Path(\"tcga-stad-features/test\"),\n",
        "    target_label=target_label,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmHJkVy7gune"
      },
      "outputs": [],
      "source": [
        "test_patient_preds_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xULFztDUr5y"
      },
      "source": [
        "Let's also compare the ROC curves.\n",
        "In the following graph, the validation ROC curve will be blue, while the one for the testing set will be orange."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rdEOEds6BCW"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "plot_single_decorated_roc_curve(\n",
        "    ax,\n",
        "    y_true=train_result.patient_preds_df[target_label] == true_class,\n",
        "    y_score=train_result.patient_preds_df[f\"{target_label}_{true_class}\"],\n",
        "    n_bootstrap_samples=1000,\n",
        ")\n",
        "\n",
        "plot_single_decorated_roc_curve(\n",
        "    ax,\n",
        "    y_true=test_patient_preds_df[target_label] == true_class,\n",
        "    y_score=test_patient_preds_df[f\"{target_label}_{true_class}\"],\n",
        "    n_bootstrap_samples=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdKvttn7UppL"
      },
      "source": [
        "As you can (hopefully) see, the confidence interval on the validation set is smaller than that on the testing data set.\n",
        "This is to be expected, since the testing set is from a site different from those used for training, while the validation set was randomly sampled from the same dataset as our training dataset.\n",
        "\n",
        "If the classifier performs significantly better on the testing set than on the validation set, it may be necessary to do further investigation:\n",
        "it may just be that the testing has more easy-to-classify samples than the training set.\n",
        "In this case we of course expect the testing set to perform better.\n",
        "If not, the algorithm performing better on the testing set may be a sign of a problem in our experimental setup.\n",
        "\n",
        "While ROC curves are one of the major endpoints for judging the quality of a classifier, they have one problem to be aware of, especially if the question of applicability of machine learning in the real world comes up.\n",
        "A ROC curve only contrasts the models sensitivity with specificity.\n",
        "This means that, as long as the model is able to separate the classes in a dataset, it will maintain a relatively high ROC.\n",
        "While this does show that the features learned by our network _are_ transferable, it can still pose problems when actually deploying our model.\n",
        "\n",
        "Often times, artifacts introduced by the way histopathological slides are prepared in a hospital can consistently affect how the network classifies a sample.\n",
        "A difference in staining for example may consistenly cause samples to be scored too highly.\n",
        "This means that for example the threshold we selected to reach a certain sensitivity may not be transferable between our validation and training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ER8wN17JGA4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "test_fpr, test_tpr, test_thresholds = roc_curve(\n",
        "    y_true=test_patient_preds_df[target_label] == true_class,\n",
        "    y_score=test_patient_preds_df[f\"{target_label}_{true_class}\"],\n",
        ")\n",
        "test_sensitivity = test_tpr\n",
        "test_specificity = 1 - test_fpr\n",
        "\n",
        "plt.plot(valid_thresholds[1:], valid_sensitivity[1:], label=\"validation sensitivity\")\n",
        "plt.plot(test_thresholds[1:], test_sensitivity[1:], label=\"test sensitivity\")\n",
        "plt.xlabel(\"threshold\")\n",
        "plt.ylabel(\"sensitivity\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONWIFor7JPSd"
      },
      "outputs": [],
      "source": [
        "plt.plot(valid_thresholds[1:], valid_specificity[1:], label=\"validation specicivity\")\n",
        "plt.plot(test_thresholds[1:], test_specificity[1:], label=\"test specicivity\")\n",
        "plt.xlabel(\"threshold\")\n",
        "plt.ylabel(\"specificity\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKHKmGq4kIhz"
      },
      "source": [
        "This concludes our hands-on for deep learning in histopathology.\n",
        "As you have seen, deep learning can be used to answer a variety of histopathological questions.\n",
        "However, while current research is promising, there are still a lot of steps remaining to make it a reliable part of medical practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysujlFNeJ1zN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ceab2f2dfff46c7a5287f0f58a3a9cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_761ce4e94ba94d688f50a6e2f6ad293d",
              "IPY_MODEL_54863c12b1ed4909aa4aa79a47ddcf4f",
              "IPY_MODEL_71393d4c10cb4276ba05dc18fb64695b"
            ],
            "layout": "IPY_MODEL_23f11454fc4e4b369d16cb20631db175"
          }
        },
        "761ce4e94ba94d688f50a6e2f6ad293d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5026eb1b0a694af8a851b1983aa2e082",
            "placeholder": "​",
            "style": "IPY_MODEL_2522bcd0dcba49649f54dbdd5488881e",
            "value": "TCGA_BRCA_10x_UNI_features.tar.gz.part_aa:  87%"
          }
        },
        "54863c12b1ed4909aa4aa79a47ddcf4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa1f4d1ec7364273a25d9ddca2b578c3",
            "max": 1393837850,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eea8f60c23dd4402aaa48e8e6774985a",
            "value": 1210096640
          }
        },
        "71393d4c10cb4276ba05dc18fb64695b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f57c33d093924e138cb5ca72aba53d15",
            "placeholder": "​",
            "style": "IPY_MODEL_da80f9053d0145d991fe742afd0e5b96",
            "value": " 1.13G/1.30G [01:53&lt;00:16, 11.0MiB/s]"
          }
        },
        "23f11454fc4e4b369d16cb20631db175": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5026eb1b0a694af8a851b1983aa2e082": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2522bcd0dcba49649f54dbdd5488881e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa1f4d1ec7364273a25d9ddca2b578c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eea8f60c23dd4402aaa48e8e6774985a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f57c33d093924e138cb5ca72aba53d15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da80f9053d0145d991fe742afd0e5b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}